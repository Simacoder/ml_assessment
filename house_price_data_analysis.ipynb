{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb5dc09",
   "metadata": {},
   "source": [
    "## House Prices - Exploratory Data Analysis and Predictive Modelling\n",
    "\n",
    "**Overview**\n",
    "                             \n",
    "In this assessment, you will perform Exploratory Data Analysis (EDA) on a Housing Prices dataset to uncover insights about the data, identify trends, and prepare it for potential modelling.\n",
    "The goal is to demonstrate your understanding of data analysis, visualization, and reporting in Python using tools such as pandas, NumPy, matplotlib, and seaborn.\n",
    "You are expected to complete the analysis in a Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc66bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759d67f5",
   "metadata": {},
   "source": [
    "## Import libraries and packages set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib seaborn scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47133ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912759fb",
   "metadata": {},
   "source": [
    "**Data Acquisition and Loading the data**\n",
    "**loading the dataset:**\n",
    "- data/train.csv\n",
    "- data/test.csv\n",
    "\n",
    "\n",
    "**The dataset was downloaded from the kaggle website challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719d125e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e363c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion \n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e95dd8",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea445b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Overview\n",
    "print(\"Training Data Overview:\")\n",
    "print(\"\\nDataset Shape:\", df_train.shape)\n",
    "print(\"\\nDataset Info:\", df_train.info())\n",
    "print(\"\\nTotal Records:\", df_train.shape[0])\n",
    "print(\"\\nTotal Features:\", df_train.shape[1])\n",
    "\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\nTesting Data Overview:\")\n",
    "print(\"\\nDataset Shape:\", df_test.shape)\n",
    "print(\"\\nDataset Info:\", df_test.info())\n",
    "print(\"\\nTotal Records:\", df_test.shape[0])\n",
    "print(\"\\nTotal Features:\", df_test.shape[1])            \n",
    "\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c790de7",
   "metadata": {},
   "source": [
    "## Data Types and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Types and Structure:\")\n",
    "print(df_train.dtypes)\n",
    "\n",
    "print(\"Feature Summary:\")\n",
    "print(\"Numerical Features:\", df_train.select_dtypes(include=[np.number]).shape[1])\n",
    "print(\"Categorical Features:\", df_train.select_dtypes(include=['object']).shape[1])\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Columns:\", len(numerical_cols))\n",
    "print(numerical_cols)\n",
    "\n",
    "print(\"Categorical Columns:\", len(categorical_cols))\n",
    "print(categorical_cols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3c031",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e3efb0",
   "metadata": {},
   "source": [
    "### Missing values analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values Analysis:\")\n",
    "\n",
    "missing_values = pd.DataFrame({\n",
    "    'Column': df_train.columns,\n",
    "    'Missing_Count': df_train.isnull().sum(),\n",
    "    'Missing_Percentage': (df_train.isnull().sum() / len(df_train)) * 100\n",
    "})\n",
    "\n",
    "missing_values = missing_values[missing_values['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "if len(missing_values) > 0:\n",
    "    print(missing_values.to_string(index=False))\n",
    "else:\n",
    "    print(\"No missing values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d43b763",
   "metadata": {},
   "source": [
    "###  Missing Values Analysis - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc64dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the test data\n",
    "if df_test is not None:\n",
    "\n",
    "    print(\"MISSING VALUES - TEST DATA\")\n",
    "    \n",
    "    \n",
    "    missing_test = pd.DataFrame({\n",
    "        'Column': df_test.columns,\n",
    "        'Missing_Count': df_test.isnull().sum(),\n",
    "        'Missing_Percent': (df_test.isnull().sum() / len(df_test)) * 100\n",
    "    })\n",
    "    \n",
    "    missing_test = missing_test[missing_test['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
    "    \n",
    "    if len(missing_test) > 0:\n",
    "        print(missing_test.to_string(index=False))\n",
    "        print(\"\\n Test data has missing values - will be handled in cleaning step\")\n",
    "    else:\n",
    "        print(\"No missing values in test data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92c968",
   "metadata": {},
   "source": [
    "### Duplicate Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82457a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate Checks:\")\n",
    "\n",
    "duplicates = df_train.duplicated().sum()\n",
    "\n",
    "print(f\"Total Duplicate Rows: {duplicates}\")\n",
    "\n",
    "if df_test is not None:\n",
    "    print(f\"Test duplicates: {df_test.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950004e6",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preparation \n",
    "- Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a46c1617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data cleaned.\n",
      "Shape after cleaning: (1460, 80)\n",
      "Remaining Missing Values: 0\n",
      "Test data cleaned.\n",
      "Shape after cleaning: (1459, 80)\n",
      "Remaining Missing Values: 0\n"
     ]
    }
   ],
   "source": [
    "def clean_and_prepare_data(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Clean and prepare data (works for both training and test sets)\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to clean\n",
    "    - is_train: Boolean, True for training data (has SalePrice), False for test\n",
    "    \n",
    "    Returns:\n",
    "    - Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Features that should be 'NA' when missing (indicate absence of feature)\n",
    "    na_features = ['Alley', 'PoolQC', 'Fence', 'MiscFeature', 'FireplaceQu', \n",
    "                   'GarageType', 'GarageFinish', 'GarageCond', 'GarageQual',\n",
    "                   'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual']\n",
    "    \n",
    "    for col in na_features:\n",
    "        if col in df_clean.columns and df_clean[col].isnull().any():\n",
    "            df_clean[col] = df_clean[col].fillna('NA')\n",
    "    \n",
    "    # Numerical columns: fill with median\n",
    "    numerical_cols_local = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Don't process ID column\n",
    "    if 'Id' in numerical_cols_local:\n",
    "        numerical_cols_local.remove('Id')\n",
    "    # Don't process SalePrice for test data\n",
    "    if not is_train and 'SalePrice' in numerical_cols_local:\n",
    "        numerical_cols_local.remove('SalePrice')\n",
    "    \n",
    "    for col in numerical_cols_local:\n",
    "        if df_clean[col].isnull().any():\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col] = df_clean[col].fillna(median_val)\n",
    "    \n",
    "    # Categorical columns: fill with mode\n",
    "    categorical_cols_local = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    for col in categorical_cols_local:\n",
    "        if df_clean[col].isnull().any():\n",
    "            mode_val = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
    "            df_clean[col] = df_clean[col].fillna(mode_val)\n",
    "    \n",
    "    # Remove Id column for modeling (but keep for test to match predictions later)\n",
    "    if 'Id' in df_clean.columns and is_train:\n",
    "        df_clean = df_clean.drop('Id', axis=1)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply the cleaning to training data\n",
    "\n",
    "df_train_clean = clean_and_prepare_data(df_train, is_train=True)\n",
    "print(\"Training data cleaned.\")\n",
    "print(\"Shape after cleaning:\", df_train_clean.shape)\n",
    "print(\"Remaining Missing Values:\", df_train_clean.isnull().sum().sum())\n",
    "\n",
    "# Apply the cleaning to test data\n",
    "if df_test is not None:\n",
    "    df_test_clean = clean_and_prepare_data(df_test, is_train=False)\n",
    "    print(\"Test data cleaned.\")\n",
    "    print(\"Shape after cleaning:\", df_test_clean.shape)\n",
    "    print(\"Remaining Missing Values:\", df_test_clean.isnull().sum().sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1e796",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "- Target Variable Analysis (SalePrice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ccc26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target Variable Analysis - SalePrice\")\n",
    "print(f\"Mean:  ${df_train_clean['SalePrice'].mean():,.2f}\")\n",
    "print(f\"Median:  ${df_train_clean['SalePrice'].median():,.2f}\")\n",
    "print(f\"Min:  ${df_train_clean['SalePrice'].min():,.2f}\")\n",
    "print(f\"Max: ${df_train_clean['SalePrice'].max():,.2f}\")\n",
    "print(f\"Standard Deviation: ${df_train_clean['SalePrice'].std():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761249ae",
   "metadata": {},
   "source": [
    "# visualise SalePrice distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7e96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize= (14, 5))\n",
    "\n",
    "# histogram\n",
    "axes[0].hist(df_train_clean['SalePrice'], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Sale Price ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Sale Price')\n",
    "axes[0].ticklabel_format(style='plain', axis='x')\n",
    "\n",
    "# Q-Q Plot (check normality)\n",
    "from scipy import stats\n",
    "stats.probplot(df_train_clean['SalePrice'], dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot: SalePrice vs Normal Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cab3a5",
   "metadata": {},
   "source": [
    "The distribution of sale prices is right skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2602aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform SalePrice\n",
    "df_train_clean[\"SalePrice_log\"] = np.log(df_train_clean.SalePrice)\n",
    "\n",
    "# Create figure with 2 subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 1️ Histogram of log-transformed SalePrice\n",
    "sns.histplot(df_train_clean[\"SalePrice_log\"], bins=50, kde=True, color='steelblue', ax=axes[0])\n",
    "axes[0].set_xlabel('Log(Sale Price)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Log-Transformed Sale Price')\n",
    "\n",
    "# 2️ Q-Q plot for log-transformed SalePrice\n",
    "stats.probplot(df_train_clean[\"SalePrice_log\"], dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot: Log(SalePrice) vs Normal Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b8964",
   "metadata": {},
   "source": [
    "### Distribution of key Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_numerical = ['GrLivArea', 'TotalBsmtSF', '1stFlrSF', 'GarageArea', \n",
    "                 'LotArea', 'YearBuilt', 'OverallQual', 'OverallCond']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(key_numerical):\n",
    "    axes[idx].hist(df_train_clean[col], bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].set_title(f'Distribution of {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b9970",
   "metadata": {},
   "source": [
    "### Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb5964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols_clean = df_train_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "correlation_matrix = df_train_clean[numerical_cols_clean].corr()\n",
    "\n",
    "\n",
    "print(\"Top 15 features most positively correlated with SalePrice:\")\n",
    "top_corr = correlation_matrix['SalePrice'].sort_values(ascending=False)[1:16]\n",
    "print(top_corr)\n",
    "\n",
    "# Heatmap of correlation matrix visualisation\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix - All Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca8bb4d",
   "metadata": {},
   "source": [
    "### Relationshiop between Top feature and SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37ad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['GrLivArea', 'OverallQual', 'TotalBsmtSF', 'GarageArea', 'YearBuilt']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(top_features):\n",
    "    axes[idx].scatter(df_train_clean[col], df_train_clean['SalePrice'], alpha=0.5, s=30, color='steelblue')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('SalePrice ($)')\n",
    "    axes[idx].set_title(f'{col} vs SalePrice')\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(df_train_clean[col], df_train_clean['SalePrice'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[idx].plot(df_train_clean[col], p(df_train_clean[col]), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "axes[-1].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1c2429",
   "metadata": {},
   "source": [
    "# Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3144c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Categorical Features Analysis\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(['Neighborhood', 'BldgType', 'HouseStyle']):\n",
    "    df_train_clean.groupby(col)['SalePrice'].mean().sort_values(ascending=False).plot(\n",
    "        kind='bar', ax=axes[idx], color='teal', alpha=0.7\n",
    "    )\n",
    "    axes[idx].set_title(f'Average SalePrice by {col}')\n",
    "    axes[idx].set_ylabel('Average Sale Price ($)')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "quality_cols = ['OverallQual', 'KitchenQual', 'ExterQual']\n",
    "for idx, col in enumerate(quality_cols):\n",
    "    if col in df_train_clean.columns:\n",
    "        df_train_clean.groupby(col)['SalePrice'].mean().plot(\n",
    "            kind='bar', ax=axes[3+idx], color='orange', alpha=0.7\n",
    "        )\n",
    "        axes[3+idx].set_title(f'Average SalePrice by {col}')\n",
    "        axes[3+idx].set_ylabel('Average Sale Price ($)')\n",
    "        axes[3+idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a73ebe",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "-  Create New Features (Applied to both Train & Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88688875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create new features for both training and test data\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # Total square footage\n",
    "    if all(col in df_eng.columns for col in ['TotalBsmtSF', '1stFlrSF', '2ndFlrSF']):\n",
    "        df_eng['TotalSF'] = df_eng['TotalBsmtSF'] + df_eng['1stFlrSF'] + df_eng['2ndFlrSF']\n",
    "    \n",
    "    # Total porch area\n",
    "    porch_cols = ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n",
    "    if all(col in df_eng.columns for col in porch_cols):\n",
    "        df_eng['TotalPorchSF'] = df_eng[porch_cols].sum(axis=1)\n",
    "    \n",
    "    # Age\n",
    "    if 'YearBuilt' in df_eng.columns:\n",
    "        df_eng['Age'] = 2024 - df_eng['YearBuilt']\n",
    "    \n",
    "    # Years renovated\n",
    "    if all(col in df_eng.columns for col in ['YearRemodAdd', 'YearBuilt']):\n",
    "        df_eng['YearsRenovated'] = df_eng['YearRemodAdd'] - df_eng['YearBuilt']\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# Apply feature engineering to training data\n",
    "df_train_eng = engineer_features(df_train_clean)\n",
    "print(\"Features engineered for training data!\")\n",
    "print(f\" New shape: {df_train_eng.shape}\")\n",
    "\n",
    "# Apply feature engineering to test data\n",
    "\n",
    "if df_test is not None:\n",
    "    df_test_eng = engineer_features(df_test_clean)\n",
    "    print(\"Features engineered for test data!\")\n",
    "    print(f\" New shape: {df_test_eng.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13517af4",
   "metadata": {},
   "source": [
    "# Encode Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6392d572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data encoded!\n",
      "  Features after encoding: 264\n",
      " Test data encoded!\n",
      "  Features after encoding: 263\n",
      "  Feature alignment check: True\n"
     ]
    }
   ],
   "source": [
    "# Get categorical columns after cleaning and feature engineering\n",
    "\n",
    "categorical_cols_train = df_train_eng.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# One-hot encode for training data\n",
    "df_train_encoded = pd.get_dummies(df_train_eng, columns=categorical_cols_train, drop_first=True)\n",
    "\n",
    "print(\"Training data encoded!\")\n",
    "print(f\"  Features after encoding: {df_train_encoded.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# One-hot encode for test data (using same categories as training)\n",
    "if df_test is not None:\n",
    "    # This ensures test data has same columns as training\n",
    "    df_test_encoded = pd.get_dummies(df_test_eng, columns=categorical_cols_train, drop_first=True)\n",
    "    \n",
    "    # Remove SalePrice column from training data if it exists\n",
    "    train_features = [col for col in df_train_encoded.columns if col != 'SalePrice']\n",
    "    \n",
    "    # Align test columns with training features only\n",
    "    missing_cols = set(train_features) - set(df_test_encoded.columns)\n",
    "    for col in missing_cols:\n",
    "        df_test_encoded[col] = 0\n",
    "    \n",
    "    # Remove extra columns from test that aren't in training features\n",
    "    extra_cols = set(df_test_encoded.columns) - set(train_features)\n",
    "    for col in extra_cols:\n",
    "        df_test_encoded = df_test_encoded.drop(col, axis=1)\n",
    "    \n",
    "    # Ensure same column order and only feature columns (no SalePrice)\n",
    "    df_test_encoded = df_test_encoded[train_features]\n",
    "    \n",
    "    print(f\" Test data encoded!\")\n",
    "    print(f\"  Features after encoding: {df_test_encoded.shape[1]}\")\n",
    "    print(f\"  Feature alignment check: {list(train_features) == list(df_test_encoded.columns)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7f0dc",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation\n",
    "\n",
    "- Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5774daa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (1460, 263)\n",
      "Training Target Shape: (1460,)\n",
      "\n",
      "Train set: 1168 records\n",
      "Validation set: 292 records\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target for training data\n",
    "X_train_full = df_train_encoded.drop('SalePrice', axis=1)\n",
    "y_train_full = df_train_encoded['SalePrice']\n",
    "\n",
    "print(f\"Training Features Shape: {X_train_full.shape}\")\n",
    "print(f\"Training Target Shape: {y_train_full.shape}\")\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} records\")\n",
    "print(f\"Validation set: {X_val.shape[0]} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058ec57",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46aae55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Features scaled using RobustScaler\n",
      "Test features scaled\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "print(\" Features scaled using RobustScaler\")\n",
    "\n",
    "# If test data exists, scale it too\n",
    "if df_test is not None:\n",
    "    X_test_scaled = scaler.transform(df_test_encoded)\n",
    "    print(\"Test features scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103c4734",
   "metadata": {},
   "source": [
    "# Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79cdd32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Training & Evaluation\n",
      "\n",
      "Training Linear Regression...\n",
      "  Train R²: 0.9753, Val R²: -0.0660, Val RMSE: $90,423\n",
      "\n",
      "Training Random Forest...\n",
      "  Train R²: 0.9994, Val R²: 0.9907, Val RMSE: $8,426\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Train R²: 0.9999, Val R²: 0.9938, Val RMSE: $6,905\n"
     ]
    }
   ],
   "source": [
    "print(\"Models Training & Evaluation\")\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_r2': train_r2,\n",
    "        'val_r2': val_r2,\n",
    "        'val_rmse': val_rmse\n",
    "    }\n",
    "    \n",
    "    print(f\"  Train R²: {train_r2:.4f}, Val R²: {val_r2:.4f}, Val RMSE: ${val_rmse:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90eede1",
   "metadata": {},
   "source": [
    "# Model Comparison & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "353d63ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model  Train R²    Val R²     Val RMSE\n",
      "Linear Regression  0.975256 -0.065960 90422.682689\n",
      "    Random Forest  0.999378  0.990744  8425.942643\n",
      "Gradient Boosting  0.999945  0.993784  6904.924238\n",
      "\n",
      " Best Model Selected: Gradient Boosting\n"
     ]
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Model': results.keys(),\n",
    "    'Train R²': [results[m]['train_r2'] for m in results.keys()],\n",
    "    'Val R²': [results[m]['val_r2'] for m in results.keys()],\n",
    "    'Val RMSE': [results[m]['val_rmse'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Select best model\n",
    "best_model_name = comparison_df.loc[comparison_df['Val R²'].idxmax(), 'Model']\n",
    "best_model = results[best_model_name]['model']\n",
    "print(f\"\\n Best Model Selected: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c3344",
   "metadata": {},
   "source": [
    "# Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d425a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATING TEST PREDICTIONS\n",
      " Predictions generated!\n",
      "  Total predictions: 1459\n",
      "  Average predicted price: $38,387\n",
      "  Price range: $36,602 - $39,363\n"
     ]
    }
   ],
   "source": [
    "if df_test is not None:\n",
    "    print(\"GENERATING TEST PREDICTIONS\")\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions = best_model.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\" Predictions generated!\")\n",
    "    print(f\"  Total predictions: {len(test_predictions)}\")\n",
    "    print(f\"  Average predicted price: ${test_predictions.mean():,.0f}\")\n",
    "    print(f\"  Price range: ${test_predictions.min():,.0f} - ${test_predictions.max():,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676ccb5",
   "metadata": {},
   "source": [
    "# Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b40becee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test IDs extracted: 1459 records\n"
     ]
    }
   ],
   "source": [
    "# Extract test IDs from original test data\n",
    "if df_test is not None:\n",
    "    test_ids = df_test['Id'].values\n",
    "    print(f\"Test IDs extracted: {len(test_ids)} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d345b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Submission file created: predictions.csv\n",
      "\n",
      "First 10 predictions:\n",
      "  Id    SalePrice\n",
      "1461 38212.500178\n",
      "1462 38256.926156\n",
      "1463 38516.067138\n",
      "1464 38537.591622\n",
      "1465 38530.335164\n",
      "1466 38537.591622\n",
      "1467 38491.885789\n",
      "1468 38249.115704\n",
      "1469 38540.579128\n",
      "1470 38212.500178\n"
     ]
    }
   ],
   "source": [
    "if df_test is not None:\n",
    "    # Create submission dataframe\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Id': test_ids,\n",
    "        'SalePrice': test_predictions\n",
    "    })\n",
    "    \n",
    "    # Save to CSV just for the control nje\n",
    "    submission_df.to_csv('predictions.csv', index=False)\n",
    "    print(f\"\\n Submission file created: predictions.csv\")\n",
    "    print(f\"\\nFirst 10 predictions:\")\n",
    "    print(submission_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1badd88",
   "metadata": {},
   "source": [
    "# Key Insights & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fd16f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY INSIGHTS & FINDINGS\n",
      "\n",
      "1. TOP PRICE DRIVERS:\n",
      "   - Overall Quality (0.79): Most important factor\n",
      "   - Ground Living Area (0.71): Size matters significantly\n",
      "   - Year Built (0.55): Newer properties preferred\n",
      "   - Garage Area (0.64): Storage space valued\n",
      "   - Total Basement SF (0.61): Additional space adds value\n",
      "\n",
      "2. MODEL PERFORMANCE:\n",
      "   - Best Model: Gradient Boosting\n",
      "   - Validation R²: 0.9938\n",
      "   - Validation RMSE: $6,905\n",
      "\n",
      "3. DATA QUALITY:\n",
      "   - Training records: 1,460\n",
      "   - Test records: 1,459 (predicted)\n",
      "   - Features engineered: 10+\n",
      "   - Missing values handled: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"KEY INSIGHTS & FINDINGS\")\n",
    "\n",
    "insights = f\"\"\"\n",
    "1. TOP PRICE DRIVERS:\n",
    "   - Overall Quality (0.79): Most important factor\n",
    "   - Ground Living Area (0.71): Size matters significantly\n",
    "   - Year Built (0.55): Newer properties preferred\n",
    "   - Garage Area (0.64): Storage space valued\n",
    "   - Total Basement SF (0.61): Additional space adds value\n",
    "\n",
    "2. MODEL PERFORMANCE:\n",
    "   - Best Model: {best_model_name}\n",
    "   - Validation R²: {results[best_model_name]['val_r2']:.4f}\n",
    "   - Validation RMSE: ${results[best_model_name]['val_rmse']:,.0f}\n",
    "\n",
    "3. DATA QUALITY:\n",
    "   - Training records: 1,460\n",
    "   - Test records: 1,459 (predicted)\n",
    "   - Features engineered: 10+\n",
    "   - Missing values handled: \n",
    "\"\"\"\n",
    "print(insights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
